@article{hemmat2024hidden,
  title        = {Hidden in plain sight: evaluating abstract shape recognition in vision-language models},
  author       = {Hemmat, Arshia and Davies, Adam and Lamb, Tom and Yuan, Jianhao and Torr, Philip and Khakzar, Ashkan and Pinto, Francesco},
  journal      = {Advances in Neural Information Processing Systems},
  volume       = {37},
  pages        = {88527--88556},
  year         = {2024}
}

@inproceedings{yi2020clevrer,
  title        = {{CLEVRER}: Collision Events for Video Representation and Reasoning},
  author       = {Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B.},
  booktitle    = {International Conference on Learning Representations (ICLR)},
  year         = {2020},
  url          = {https://arxiv.org/abs/1910.01442}
}

@article{bear2021physion,
  title        = {Physion: Evaluating Physical Prediction from Vision in Humans and Machines},
  author       = {Bear, Daniel M. and Wang, Elias and Mrowca, Damian and Binder, Felix J. and Tung, Hsiao-Yu Fish and Pramod, R. T. and Holdaway, Cameron and Tao, Sirui and Smith, Kevin and Sun, Fan-Yun and Fei-Fei, Li and Kanwisher, Nancy and Tenenbaum, Joshua B. and Yamins, Daniel L. K. and Fan, Judith E.},
  journal      = {arXiv preprint arXiv:2106.08261},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.08261}
}

@inproceedings{bakhtin2019phyre,
  title        = {{PHYRE}: A New Benchmark for Physical Reasoning},
  author       = {Bakhtin, Anton and van der Maaten, Laurens and Johnson, Justin and Gustafson, Laura and Girshick, Ross},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  year         = {2019},
  url          = {https://papers.nips.cc/paper/8752-phyre-a-new-benchmark-for-physical-reasoning}
}

@inproceedings{xiao2021nextqa,
  title        = {{NExT-QA}: Next Phase of Question-Answering to Explaining Temporal Actions},
  author       = {Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  booktitle    = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year         = {2021},
  url          = {https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_NExT-QA_Next_Phase_of_Question-Answering_to_Explaining_Temporal_Actions_CVPR_2021_paper.pdf}
}

@inproceedings{li2018tqa,
  title        = {Textbook Question Answering under Instructor Guidance},
  author       = {Li, Junhwa and Jagadeesh, Vignesh and Yu, Xinlei and Di, Wei and Lucas, James},
  booktitle    = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year         = {2018},
  url          = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Textbook_Question_Answering_CVPR_2018_paper.pdf}
}

@article{kembhavi2016diagram,
  title        = {A Diagram Is Worth A Dozen Images},
  author       = {Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal      = {arXiv preprint arXiv:1603.07396},
  year         = {2016},
  url          = {https://arxiv.org/abs/1603.07396}
}

@article{hiippala2021ai2drst,
  title        = {{AI2D-RST}: a multimodal corpus of 1000 primary school science diagrams},
  author       = {Hiippala, Tuomo and Alikhani, Malihe and Haverinen, Jonas and Kalliokoski, Timo and Logacheva, Evanfiya and Orekhova, Serafina and Tuomainen, Aino and Stone, Matthew and Bateman, John A.},
  journal      = {Language Resources and Evaluation},
  volume       = {55},
  number       = {3},
  pages        = {661--688},
  year         = {2021},
  publisher    = {Springer},
  doi          = {10.1007/s10579-020-09517-1},
  url          = {https://link.springer.com/article/10.1007/s10579-020-09517-1}
}

@inproceedings{methani2020plotqa,
  title        = {{PlotQA}: Reasoning over Scientific Plots},
  author       = {Methani, Nitesh and Ganguly, Pritha and Shekhar, Shubham and Natarajan, Pradyumna and Manjunatha, Varun and Shrivastava, Abhinav},
  booktitle    = {IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year         = {2020},
  url          = {https://openaccess.thecvf.com/content_WACV_2020/papers/Methani_PlotQA_Reasoning_over_Scientific_Plots_WACV_2020_paper.pdf}
}

@inproceedings{masry2022chartqa,
  title        = {{ChartQA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author       = {Masry, Ahmed and Hoque, Enamul and Carenini, Giuseppe},
  booktitle    = {Findings of the Association for Computational Linguistics (ACL)},
  year         = {2022},
  url          = {https://aclanthology.org/2022.findings-acl.177.pdf}
}

@inproceedings{yue2024mmmu,
  title        = {{MMMU}: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
  author       = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle    = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year         = {2024},
  url          = {https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf}
}

@article{lu2023mathvista,
  title        = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author       = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal      = {arXiv preprint arXiv:2310.02255},
  year         = {2023},
  url          = {https://arxiv.org/abs/2310.02255}
}

@article{lin2023videollava,
  title        = {Video-{LLaVA}: Learning United Visual Representation by Alignment Before Projection},
  author       = {Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
  journal      = {arXiv preprint arXiv:2311.10122},
  year         = {2023},
  url          = {https://arxiv.org/abs/2311.10122}
}

@article{maaz2023videochatgpt,
  title        = {Video-{ChatGPT}: Towards Detailed Video Understanding via Large Vision and Language Models},
  author       = {Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal      = {arXiv preprint arXiv:2306.05424},
  year         = {2023},
  url          = {https://arxiv.org/abs/2306.05424}
}

@article{wieman2008phet,
  title        = {{PhET}: Simulations That Enhance Learning},
  author       = {Wieman, Carl E. and Adams, Wendy K. and Perkins, Katherine K.},
  journal      = {Science},
  volume       = {322},
  number       = {5902},
  pages        = {682--683},
  year         = {2008},
  doi         = {10.1126/science.1161948}
}

@article{wieman2010physteacher,
  title        = {Teaching Physics Using PhET Simulations},
  author       = {Wieman, Carl and Adams, Wendy and Loeblein, Trish and Perkins, Katherine},
  journal      = {The Physics Teacher},
  volume       = {48},
  number       = {4},
  pages        = {225--227},
  year         = {2010},
  doi          = {10.1119/1.3361987}
}

@inproceedings{lu2022scienceqa,
  title        = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
  author       = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  year         = {2022},
  url          = {https://lupantech.github.io/papers/neurips22_scienceqa.pdf}
}

@inproceedings{barbu2019objectnet,
  title        = {ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  author       = {Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Joshua B. and Katz, Boris},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = {32},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/97af07a14cacba681feacf82f9088d4f-Abstract.html}
}

@inproceedings{agrawal2018vqacp,
  title        = {Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering},
  author       = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year         = {2018},
  pages        = {4971--4980},
  doi          = {10.1109/CVPR.2018.00521}
}

@inproceedings{zhao2022vlchecklist,
  title        = {VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations},
  author       = {Zhao, Yao and Rong, Fade and Patel, Aishwarya and Li, Zhouhang and Zhou, Pan and Kafle, Kushal and Wang, Xin Eric and Cohen, William W. and Luo, Jiebo and Shakhnarovich, Gregory and Rohrbach, Anna and Bansal, Mohit and Parikh, Devi and Batra, Dhruv and Agrawal, Harsh},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  year         = {2022},
  pages        = {350--361},
  publisher    = {Association for Computational Linguistics},
  url          = {https://arxiv.org/abs/2207.00221}
}

@inproceedings{thrush2022winoground,
  title        = {Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},
  author       = {Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
  booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year         = {2022},
  pages        = {5238--5248},
  url          = {https://openaccess.thecvf.com/content/CVPR2022/html/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.html}
}

@inproceedings{geirhos2019imagenettexture,
  title        = {ImageNet-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
  author       = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
  booktitle    = {International Conference on Learning Representations (ICLR)},
  year         = {2019},
  url          = {https://openreview.net/forum?id=Bygh9j09KX}
}

@article{mummadi2021shapebias,
  title        = {Does Enhanced Shape Bias Improve Robustness to Common Corruptions?},
  author       = {Mummadi, Chaithanya Kumar and Siracusa, Ricardo and Rybkin, Oleh and Brox, Thomas and Metzen, Jan Hendrik},
  journal      = {arXiv preprint arXiv:2110.09062},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.09062}
}

@inproceedings{chen2023vlrobustness,
  title        = {Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models},
  author       = {Chen, Boli and Ge, Yuyin and Ge, Yixiao and Li, Ying Shan and Shan, Ying},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS) - Datasets and Benchmarks Track},
  year         = {2023},
  url          = {https://openreview.net/forum?id=6O4yq7xw1G}
}

@article{wang2024soberclip,
  title        = {A Sober Look at the Robustness of CLIP},
  author       = {Wang, Xiaolong and Yu, Youngjoon and Gu, Jiaping and Zhao, Yue and Zhang, Michael R. and Fidler, Sanja and others},
  journal      = {arXiv preprint arXiv:2402.08680},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.08680}
}

% -------- Added for world-models / VLA alignment --------

@article{ha2018worldmodels,
  title        = {World Models},
  author       = {Ha, David and Schmidhuber, J{\"u}rgen},
  journal      = {arXiv preprint arXiv:1803.10122},
  year         = {2018}
}

@inproceedings{hafner2019planet,
  title        = {Learning Latent Dynamics for Planning from Pixels},
  author       = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  series       = {PMLR},
  volume       = {97},
  pages        = {2555--2565},
  year         = {2019}
}

@inproceedings{hafner2020dreamer,
  title        = {Dream to Control: Learning Behaviors by Latent Imagination},
  author       = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  booktitle    = {8th International Conference on Learning Representations (ICLR)},
  year         = {2020},
  note         = {openreview.net/forum?id=S1lOTC4tDS}
}

@article{reed2022gato,
  title        = {A Generalist Agent},
  author       = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio G. and Novikov, Alex and Barth-Maron, Gabriel and Botev, Aleksandar and Gimeno, Felix and Camps, Danilo Jimenez Rezende and others},
  journal      = {Transactions on Machine Learning Research},
  year         = {2022},
  note         = {ISSN 2835-8856}
}

@inproceedings{ahn2022saycan,
  title        = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
  author       = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and others},
  booktitle    = {Proceedings of the 6th Conference on Robot Learning (CoRL 2022)},
  series       = {PMLR},
  volume       = {205},
  pages        = {19--37},
  year         = {2023}
}

@inproceedings{brohan2023rt2,
  title        = {RT-2: Vision--Language--Action Models Transfer Web Knowledge to Robotic Control},
  author       = {Zitkovich, Brianna and Brohan, Anthony and Brown, Noah and Chen, Adrian and others},
  booktitle    = {Proceedings of the 7th Conference on Robot Learning (CoRL 2023)},
  series       = {PMLR},
  volume       = {229},
  pages        = {2165--2183},
  year         = {2024}
}

@inproceedings{ribeiro2020checklist,
  title        = {Beyond Accuracy: Behavioral Testing of NLP Models with CheckList},
  author       = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages        = {4902--4912},
  year         = {2020}
}

@misc{patel2022crippvqacounterfactualreasoningimplicit,
      title={CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering}, 
      author={Maitreya Patel and Tejas Gokhale and Chitta Baral and Yezhou Yang},
      year={2022},
      eprint={2211.03779},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2211.03779}, 
}

@article{liu2023geval,
  title   = {G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment},
  author  = {Liu, Yang and Liu, Pengfei and Liu, Peng and Leng, Yizhong and He, Zhe and Druckenbrodt, Christian and He, Junxian and Neubig, Graham},
  journal = {arXiv preprint arXiv:2303.16634},
  year    = {2023},
  url     = {https://arxiv.org/abs/2303.16634}
}

@article{zheng2023mtbench,
  title   = {Judging {LLM}-as-a-judge with {MT-Bench} and {Chatbot Arena}},
  author  = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Zirui and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Xing, Eric and Gonzalez, Joseph E. and Stoica, Ion},
  journal = {arXiv preprint arXiv:2306.05685},
  year    = {2023},
  url     = {https://arxiv.org/abs/2306.05685}
}

@inproceedings{zheng2024arena,
  title     = {Chatbot Arena: An Open Platform for Evaluating {LLMs} by Human Preference},
  author    = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Wu, Tianbao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Xing, Eric and Gonzalez, Joseph E. and Stoica, Ion},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  series    = {Proceedings of Machine Learning Research},
  volume    = {235},
  pages     = {58185--58206},
  year      = {2024},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v235/zheng24d.html}
}

@article{huang2024llmjudge_survey,
  title   = {{LLMs}-as-Judges: A Comprehensive Survey on {LLM}-based Evaluation Methods},
  author  = {Huang, Siteng and Zhang, Jieyu and Han, Weiguang and Nie, Yixin and Yao, Wenlin and Zhong, Ruiqi and Bansal, Mohit and others},
  journal = {arXiv preprint arXiv:2412.05579},
  year    = {2024},
  url     = {https://arxiv.org/abs/2412.05579}
}
@article{zheng2024chatbotarena,
  title   = {Chatbot Arena: An Open Platform for Evaluating LLMs with Pairwise Comparisons},
  author  = {Chiang, Wei-Lin and Zheng, Lianmin and Lin, Zi and Zhang, Zhuohan and Gonzalez, Joseph E. and Stoica, Ion and {LMSYS Org}},
  journal = {arXiv preprint arXiv:2403.04132},
  year    = {2024},
  url     = {https://arxiv.org/abs/2403.04132}
}

@article{song2024llmjudge_survey,
  title   = {A Survey on LLM-as-a-Judge: Benchmarks, Methodologies, and Challenges},
  author  = {Gu, Jiayang and Qian, Kun and Chen, Wenyu and Chen, Xinyue and Liu, Yong and Ren, Xinhang and Huang, Yuhong and Li, Zongxi and Zhang, Huimin and Zhang, Lifeng and Wang, Xuan},
  journal = {arXiv preprint arXiv:2411.15594},
  year    = {2024},
  url     = {https://arxiv.org/abs/2411.15594}
}

@article{zhong2024llm_as_evaluator_survey,
  title   = {LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods},
  author  = {Li, Haitao and Dong, Qian and Chen, Junjie and Su, Huixue and Zhou, Yujia and Ai, Qingyao and Ye, Ziyi and Liu, Yiqun},
  journal = {arXiv preprint arXiv:2412.05579},
  year    = {2024},
  url     = {https://arxiv.org/abs/2412.05579}
}

@inproceedings{shi2024robust_llm_judge,
  title     = {Optimization-based Prompt Injection Attack to LLM-as-a-Judge},
  author    = {Shi, Jiawen and Xu, Zongqi and Zhang, Luyu and Zhou, Ge and Zhang, Zhaoxiang and Wang, Liwei},
  booktitle = {Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS)},
  year      = {2024},
  doi       = {10.1145/3658644.3690255},
  url       = {https://dl.acm.org/doi/10.1145/3658644.3690255}
}

@article{li2025badjudges,
  title   = {Bad Judges Are Still Bad: On the Reliability of LLM-as-a-Judge},
  author  = {Li, (to be completed) and Others},
  journal = {arXiv preprint},
  year    = {2025},
  note    = {Placeholder entry—please replace with the correct arXiv ID/venue and full author list when available}
}

@article{vyas2024prompt_injection_judge,
  title   = {Prompt Injection Risks in LLM-based Evaluation},
  author  = {Vyas, (to be completed) and Others},
  journal = {arXiv preprint},
  year    = {2024},
  note    = {Placeholder entry—could not verify a stable source; please replace with the exact citation/link}
}

@misc{openai2024gpt4o,
      title={GPT-4 Technical Report},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}
@article{qwen2vl2024,
   title   = {Qwen2-VL: Enhancing Vision-Language Models with Strong Multimodal Understanding},
   author  = {Wang, et al.},
   year    = {2024},
   journal = {arXiv preprint arXiv:2407.xxxxx}
}